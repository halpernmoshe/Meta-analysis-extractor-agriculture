System/Study,Year,Journal,Domain,Models Used,N Papers Tested,Data Type,Primary Accuracy Metric,Quantitative Accuracy,Consensus/Multi-model?,Cost per Paper,Key Limitation
"Gartlehner et al. (2024)",2024,Research Synthesis Methods,Clinical (RCTs),"Claude 2 (zero-shot)",10,Categorical (16 data types),"96.3% accuracy (160 elements)",Not tested separately,No (single model),Not reported,"Small sample (10 papers); categorical data only; no quantitative outcome extraction"
"Gougherty & Clipp (2024)",2024,npj Biodiversity,Ecology (disease reports),"Google text-bison-001",100,Both (categorical + quantitative),">90% on categorical; 23.8% on quantitative","23.8% accuracy on pathogen incidence",No (single model),Not reported,"Poor quantitative accuracy (23.8%); hallucinated 100% incidence for 53% of missing values; single older-generation model"
"Gartlehner et al. (2025)",2025,Annals of Internal Medicine,Clinical (6 systematic reviews),"Claude 2.1, 3.0 Opus, 3.5 Sonnet",63,Both (9341 data elements),"91.0% accuracy; 98.9% precision; 89.4% recall",Not separated from categorical,Partially (multiple Claude versions tested sequentially),Not reported,"Concordance with humans only 77.2%; tested Claude versions sequentially rather than true multi-model consensus"
"Jensen et al. (2025)",2025,PLoS ONE,Clinical (exercise/fall risk),"ChatGPT-4o",11,Both (484 data points),"92.4% accuracy; 5.2% false data rate",Not separated from categorical,No (single model),Not reported,"Small sample (11 papers); single domain (exercise); used as second rater not primary extractor; reproducibility dropped to 77.2% for unreported data"
"Khan et al. (2025)",2025,JAMIA,Clinical (living systematic reviews),"GPT-4-turbo + Claude-3-Opus",22 publications (10 trials),Both (23 variables: trial + population + outcomes),"94% accuracy (concordant pairs); 0.25% hallucination when concordant","Included in overall 94%; numerical required exact match",Yes (2-model collaborative workflow),Not reported,"Small sample (10 trials); concordant accuracy only; discordant responses had 41-50% accuracy; 27-41% hallucination when models disagreed"
"Li et al. (2025)",2025,arXiv preprint,Clinical (3 medical domains),"GPT-4o-mini, Gemini-2.0-flash, Grok-3",58,Both (statistics + quality + study info),"Precision 0.75-0.95; Recall 0.21-0.84","Statistics recall 0.21-0.76; precision 0.81-0.95",Tested (model ensemble gave +5.9% recall),Not reported,"High precision but poor recall across all models; recall as low as 21% for statistical results; no cost analysis"
"Sallam et al. (2025)",2025,Research Synthesis Methods,Clinical (CBT for insomnia),"GPT-4o, o3",290 (dev) + 15 (validation),Both,"GPT-4o: 72.6% accuracy; o3: 75.3% accuracy","Included in overall accuracy",No (single model per run),Not reported,"Open-source but single-model; moderate accuracy (72-75%); large development set but small validation (15 papers)"
"Poser et al. (2026)",2026,Frontiers in AI,Clinical (Multiple Sclerosis),"Claude 3.7 Sonnet + Gemini 2.0 Pro + o3-mini",30 clinical reports,Categorical (19 clinical variables),"1.48% true-error rate (comparable to neurologists ~2%)",Not applicable (categorical only),Yes (3-LLM consensus),Not reported,"Categorical clinical variables only; small sample (30 reports); German-language reports; no quantitative outcome extraction"
"This study",2026,--,Plant science (CO2 + minerals; Zn biofortification),"Claude Sonnet 4 + Kimi K2.5 + Gemini 3 Flash tiebreaker","54 (46 Loladze + 8 Hui)",Quantitative (continuous effect sizes),"r=0.695 (Loladze); r=0.957 (Hui); TOST equivalence p=0.004","MAE=7.7% (Loladze); MAE=5.15% (Hui); 85-100% direction agreement; 91-93% within 20%","Yes (3-model consensus with 2-of-3 voting; 73% gain over best single model)","~$0.37/paper ($17 total for 46 papers)","Plant science only (2 datasets); 83% element capture rate; wide observation-level limits of agreement (+-30pp); variance extraction lags behind means"
